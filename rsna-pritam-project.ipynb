{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":13451,"databundleVersionId":1188070,"sourceType":"competition"},{"sourceId":762454,"sourceType":"datasetVersion","datasetId":396552},{"sourceId":20989393,"sourceType":"kernelVersion"},{"sourceId":22701660,"sourceType":"kernelVersion"},{"sourceId":23446541,"sourceType":"kernelVersion"},{"sourceId":55994290,"sourceType":"kernelVersion"}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pydicom\n!pip install seaborn\n\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\nimport matplotlib.pylab as plt\nimport os\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:44:50.594800Z","iopub.execute_input":"2024-01-10T11:44:50.595770Z","iopub.status.idle":"2024-01-10T11:45:01.773042Z","shell.execute_reply.started":"2024-01-10T11:44:50.595729Z","shell.execute_reply":"2024-01-10T11:45:01.772101Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PATH=\"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection\"\n!ls ../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:45:01.774164Z","iopub.execute_input":"2024-01-10T11:45:01.774675Z","iopub.status.idle":"2024-01-10T11:45:02.735346Z","shell.execute_reply.started":"2024-01-10T11:45:01.774649Z","shell.execute_reply":"2024-01-10T11:45:02.734262Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"stage_2_sample_submission.csv  stage_2_test  stage_2_train  stage_2_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(join(PATH,'stage_2_train.csv'))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:45:02.738089Z","iopub.execute_input":"2024-01-10T11:45:02.738404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Sub_type'] = train['ID'].str.split(\"_\", n = 2, expand = True)[2]\ntrain['PatientID'] = train['ID'].str.split(\"_\", n = 2, expand = True)[1]\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_training_patients=len(os.listdir(\"/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train\"))\nnum_of_testing_patients=len(os.listdir(\"/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_test\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting amount of training and testing data\nlabels = 'Training', 'Testing'\nsizes = [num_of_training_patients, num_of_testing_patients]\nexplode = (0, 0.1)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\nax.axis('equal')\nax.set_title('Training and Testing Data')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparing 0 labels to 1 labels\nprint(train.Label.value_counts())\nsns.countplot(x='Label', data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of each subtype labeled as 1\nsubtype_counts = train.groupby(\"Sub_type\").Label.value_counts().unstack()\nsubtype_counts = subtype_counts.loc[:, 1]\nsubtype_counts\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20, 8))\n\nsns.countplot(x=\"Sub_type\", hue=\"Label\", data=train)\n\nplt.title(\"Total Images by Subtype\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:**\n* The samples labeled with 0s are too much when compared to the samples labeled with 1s for each subtype.","metadata":{}},{"cell_type":"code","source":"labels =  'epidural','intraparenchymal','intraventricular','subarachnoid','subdural'\nsizes = [subtype_counts[1],subtype_counts[2],subtype_counts[3],subtype_counts[4],subtype_counts[5]]\nexplode = (0, 0.1)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90)\nax.axis('equal')\nax.set_title('Subtypes')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes:**\n*  We can see here that the data is not balanced, some subtypes have few examples, and that will make it hard to train the model to detectthose subtypes(epidural for example).\n*  Data augmentation techniques will be required to perform the IH detection. Or random sampling can be used too, in such a way that the number of positive patients are equal to the number of negative patients. And as the data is so big I suggest to use subset of it, and the choosen subset have to balanced. ","metadata":{}},{"cell_type":"code","source":"traindf=train.copy()\ntraindf[['ID', 'Image', 'Diagnosis']] = traindf['ID'].str.split('_', expand=True)\ntraindf = traindf[['Image', 'Diagnosis', 'Label']]\ntraindf.drop_duplicates(inplace=True)\ntraindf = traindf.pivot(index='Image', columns='Diagnosis', values='Label').reset_index()\ntraindf['Image'] = 'ID_' + traindf['Image']\ntraindf.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cases with more than one IH subtype detected in the training dataset\nx=[]\nfor n in range(6):\n    many = traindf[traindf[['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].sum(1) == n].copy()\n    x.append(len(many))\n    print('Number of hemorrhages: {}, amount of such images: {}, fraction: {:.3f}%'.format(n, len(many), 100 * len(many) / len(traindf)))\n    print(x[n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=['0','1','2','3','4','5']\nfig, ax = plt.subplots()\nsns.barplot(x=list(y[1:]), y=list(x[1:]), ax=ax)\nax.set_title(\"the number of images for each class\")\nax.set_xlabel(\"class\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-image\n\nfrom skimage.io import imread_collection\nimport skimage.io\nimport skimage.color\nimport skimage.transform\nfrom platform import python_version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:**\nIn most of the cases where IH is detected we have one subtype hemorhage detected. ","metadata":{}},{"cell_type":"code","source":"# extract filenames from the folder of images\nfilenames = []\nfor root, dirs, files in os.walk('../input/rsna-hemorrhage-jpg/train_jpg/train_jpg'):\n    for file in files:\n        if file.endswith('.jpg'):\n            filenames.append(file)\n            \n# should be the same as the images imported\nlen(filenames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_dir = '../input/rsna-hemorrhage-jpg/train_jpg/train_jpg/*.jpg'\n\n# Create a collection with the available images\nimages = imread_collection(col_dir)\n\nlen(images)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the first image\nplt.figure()\nplt.imshow(images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n\nprint(images[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(images[0].shape)\nprint(images[1].shape)\nprint(images[2].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only the first 5000 images\nimages_trn = images[:2000]\nprint(len(images_trn))\nimages_val = images[20000:22000]\nprint(len(images_val))\nimages_tst = images[25000:30000]\nprint(len(images_tst))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_arr_trn = skimage.io.collection.concatenate_images(images_trn)\nimages_arr_val = skimage.io.collection.concatenate_images(images_val)\nimages_arr_tst = skimage.io.collection.concatenate_images(images_tst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyarrow\n\nlabels = pd.read_feather('../input/rsna-hemorrhage-jpg/meta/meta/labels.fth')\n\n#manipulate the filenames list, stripping the .jpg at the end\nidstosearch = [item.rstrip(\".jpg\") for item in filenames]\n\n#now search the \"ID\" column for ids that correspond to our filenames\n#made the reduced dataframe \"labels2\" for now\nlabels2 = labels[labels['ID'].isin(idstosearch)]\nlabels2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = labels2.iloc[:, 1]\nprint(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_trn = labels[:2000]\nprint(len(labels_trn))\nlabels_val = labels[20000:22000]\nprint(len(labels_val))\nlabels_tst = labels[25000:30000]\nprint(len(labels_tst))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(labels_trn))\nprint(labels_trn.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform labels into array\nlabels_trn = pd.Series.to_numpy(labels_trn)\nprint(len(labels_trn))\nlabels_val = pd.Series.to_numpy(labels_val)\nprint(len(labels_val))\nlabels_tst = pd.Series.to_numpy(labels_tst)\nprint(len(labels_tst))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications import resnet50\n\nmodel = resnet50.ResNet50(weights=\"imagenet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resize all images \n\nimages_final = []\n\nfor i in range(len(images_arr_trn)):\n  image_rescaled = skimage.transform.resize(images_arr_trn[i], (224, 224, 3))\n  images_final.append(image_rescaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.transform import resize\n# Resize validation images to (224, 224, 3)\nimages_val = [resize(image, (224, 224, 3)) for image in images_val]\n\n# Resize test images to (224, 224, 3)\nimages_tst = [resize(image, (224, 224, 3)) for image in images_tst]\n\n# Resize all images to 224x224\nimages_final_resized = [resize(image, (224, 224, 3)) for image in images_final]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MODEL**","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom skimage.transform import resize\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Base model (ResNet50 without top classification layers, using pre-trained weights)\nbase_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Global average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n\n# Fully connected layer\nx = Dense(256, activation='relu')(x)\n\n# Output layer with sigmoid activation for binary classification (assuming binary classification)\npredictions = Dense(1, activation='sigmoid')(x)\n\n# Combine base model and top layers\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Freeze layers from the ResNet50 base model\nfor layer in base_model.layers:\n    layer.trainable = False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])    \n    \n    \n# Defining a loss object and an optimizer\n#loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n#optimizer = tf.keras.optimizers.Adam()    \n    \n# Compile the model\n#model.compile(optimizer=Adam(lr=0.001), loss=loss_object, metrics=['accuracy'])\n\n\n\n\n# Display model summary\n#model.summary()\n\n#visualization of the model architechture\n#tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training  setup","metadata":{}},{"cell_type":"code","source":"#THE BELOW TRAINING FUNCTION IS BETTER\n# Train the model\n\"\"\"history = model.fit(\n    np.array(images_final_resized),  # Input images\n    labels_trn,  # Target labels\n    epochs=2,\n    batch_size=64,\n    validation_data=(np.array(images_val), labels_val)\n)\n\"\"\"\n# Evaluate the model on test data\n#test_loss, test_accuracy = model.evaluate(np.array(images_tst), labels_tst)\n#print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TRAINING**","metadata":{}},{"cell_type":"code","source":"\"\"\"epochs=2\n# Train the model\nfor epoch in range(epochs):\n    history = model.fit(\n        np.array(images_final),  # Input images\n        labels_trn,  # Target labels\n        epochs=1,\n        batch_size=32,\n        validation_data=(np.array(images_val), labels_val)\n    )\n\n    # Evaluate on validation data after each epoch\n    y_val_pred = model.predict(np.array(images_val))\n    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n\n    val_accuracy = accuracy_score(labels_val, y_val_pred_binary)\n    val_precision = precision_score(labels_val, y_val_pred_binary)\n    val_recall = recall_score(labels_val, y_val_pred_binary)\n    val_f1 = f1_score(labels_val, y_val_pred_binary)\n\n    print(f\"Epoch {epoch + 1} - Validation Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\"\"\"# Train the model and collect history\nhistory = model.fit(\n    np.array(images_final),\n    labels_trn,\n    epochs=epochs,\n    batch_size=32,\n    validation_data=(np.array(images_val), labels_val)\n)\n\"\"\"\n# Define initial variables and callbacks\nbest_val_accuracy = 0.0\ncheckpoint_path = 'best_model_weights.h5'\n\n# Define a callback to save the best model weights based on validation accuracy\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n\n\n# Give the Epoch number\nepochs=15\n\n\n# Train the model\nfor epoch in range(epochs):\n    history = model.fit(\n        np.array(images_final),  # Input images\n        labels_trn,  # Target labels\n        epochs=1,\n        batch_size=64,\n        validation_data=(np.array(images_val), labels_val)\n    )\n\n    # Evaluate on validation data after each epoch\n    y_val_pred = model.predict(np.array(images_val))\n    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n\n    val_accuracy = accuracy_score(labels_val, y_val_pred_binary)\n    \n    \n    # Check if validation accuracy has improved\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        print(f\"Epoch {epoch + 1} - Validation Accuracy Improved: {best_val_accuracy:.4f}. Saving model weights.\")\n        model.save_weights(checkpoint_path)  # Save the model weights\n\n    \n    \n    val_precision = precision_score(labels_val, y_val_pred_binary)\n    val_recall = recall_score(labels_val, y_val_pred_binary)\n    val_f1 = f1_score(labels_val, y_val_pred_binary)\n\n    \n    \n    \n    print(f\"Epoch {epoch + 1} - Validation Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")\n\n\n\n\n# Plotting accuracy and loss for each epoch\nplt.figure(figsize=(12, 5))\n\n# Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Evaluate the model on test data after training\n# ... (rest of your code for evaluation)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing","metadata":{}},{"cell_type":"code","source":"\n# Evaluate the model on test data after training\ny_test_pred = model.predict(np.array(images_tst))\ny_test_pred_binary = (y_test_pred > 0.5).astype(int)\n\ntest_accuracy = accuracy_score(labels_tst, y_test_pred_binary)\ntest_precision = precision_score(labels_tst, y_test_pred_binary)\ntest_recall = recall_score(labels_tst, y_test_pred_binary)\ntest_f1 = f1_score(labels_tst, y_test_pred_binary)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HAVE TO SAVE THE WEIGHTS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HAVE TO GIVE THE WEIGHTS TO THE MODEL AND GIVE THE IMAGE TO THE MODEL TO SEE RESULTS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_val = []\n\nfor i in range(len(images_arr_val)):\n  image_rescaled = skimage.transform.resize(images_arr_val[i], (224, 224, 3))\n  images_val.append(image_rescaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_val = skimage.io.collection.concatenate_images(images_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate model\ntest_loss, test_acc = model.evaluate(images_val, labels_val, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}